{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLeiw-l6BVG5",
        "outputId": "add61ec1-d9fe-4c6f-97e9-05713db0708b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ **Importing Helper Modules**\n",
        "\n",
        "import torch  # ğŸ§  Core PyTorch library for tensor operations and neural networks\n",
        "import torch.nn as nn  # ğŸ—ï¸ Neural network components (layers, loss functions)\n",
        "import torch.optim as optim  # âš™ï¸ Optimization algorithms (SGD, Adam, etc.)\n",
        "import torchvision  # ğŸ¨ Computer vision utilities and datasets\n",
        "import torchvision.transforms as transforms  # ğŸ–¼ï¸ Data transformations (normalization, augmentation)\n",
        "from torch.utils.data import DataLoader  # ğŸšš For loading and batching data\n",
        "import matplotlib.pyplot as plt  # ğŸ“Š Visualization for losses and accuracies\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FUtsc0UlBVG7"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_mnist_data(batch_size=64):\n",
        "    \"\"\"\n",
        "    ğŸ“¦ Load and preprocess the MNIST dataset.\n",
        "    ğŸ“œ Returns: train_loader and test_loader ğŸ¯\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # ğŸ”„ Resize images to 224x224 pixels\n",
        "        transforms.ToTensor(),  # ğŸ”„ Convert images to tensors ğŸ“Š\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # ğŸŒˆ Convert grayscale to RGB\n",
        "        transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))  # âš–ï¸ Normalize for RGB\n",
        "        #transforms.Normalize((0.1307,), (0.3081,)) #normalize for grayscale\n",
        "    ])\n",
        "\n",
        "    # ğŸ› ï¸ Load MNIST training and test datasets ğŸ–¼ï¸\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader  # ğŸšš Return the loaders ğŸ“¦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AQpYDtVLBVG7"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ **Part 2: Custom Dropout Implementation**\n",
        "\n",
        "class CustomDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    ğŸ› ï¸ TODO: Implement custom dropout layer ğŸ¯\n",
        "\n",
        "    ğŸ“œ **Requirements:**\n",
        "    1ï¸âƒ£ Initialize with **dropout probability** `p` ğŸ²\n",
        "    2ï¸âƒ£ Implement **forward pass** with proper scaling ğŸ”„\n",
        "    3ï¸âƒ£ **Only drop** units during **training** (`self.training` flag) ğŸ‹ï¸â€â™‚ï¸\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super(CustomDropout, self).__init__()\n",
        "        # ğŸ² Store dropout probability (p between 0 and 1)\n",
        "        self.p = p\n",
        "        pass  # ğŸš§ Initialization complete! Time to implement the logic ğŸ› ï¸\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ğŸ”„ **TODO: Implement forward pass**\n",
        "        if self.training:  # ğŸ‹ï¸â€â™‚ï¸ Drop units only during training mode\n",
        "            mask = torch.bernoulli(torch.ones_like(x) * (1 - self.p))\n",
        "            x = x * mask / (1 - self.p)\n",
        "            # ğŸš§ Work in progress! Apply dropout logic ğŸ§ª\n",
        "        return x  # ğŸ”„ Return the (possibly dropped) output âœ¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DNpn47FQBVG7"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ **Part 3: Custom BatchNorm2d Implementation**\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "    \"\"\"\n",
        "    ğŸ› ï¸ TODO: Implement custom 2D batch normalization ğŸ”„\n",
        "\n",
        "    ğŸ“œ **Requirements:**\n",
        "    1ï¸âƒ£ Initialize **running mean**, **variance**, **gamma (scale)**, and **beta (shift)** âš–ï¸\n",
        "    2ï¸âƒ£ Implement **forward pass** with proper normalization âœ¨\n",
        "    3ï¸âƒ£ Track **running statistics** during training ğŸ“Š\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(CustomBatchNorm2d, self).__init__()\n",
        "        # ğŸ› ï¸ **TODO: Initialize parameters and buffers**\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta  = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "        # ğŸš§ Work in progress ğŸš€\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ğŸ”„ **TODO: Implement forward pass for batch normalization**\n",
        "        # Steps:|\n",
        "        # 1ï¸âƒ£ Calculate batch mean and variance ğŸ“Š\n",
        "        # 2ï¸âƒ£ Normalize the input ğŸ¯\n",
        "        # 3ï¸âƒ£ Apply learnable parameters (gamma and beta) âš™ï¸\n",
        "        # 4ï¸âƒ£ Update running statistics during training ğŸ‹ï¸â€â™‚ï¸\n",
        "        if self.training:\n",
        "            batch_mean = x.mean(dim=[0, 2, 3], keepdim=True)  # Mean over N, H, W\n",
        "            batch_var = x.var(dim=[0, 2, 3], unbiased=False, keepdim=True)  # Variance over N, H, W\n",
        "\n",
        "            # Normalize input\n",
        "            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
        "\n",
        "            # Scale and shift using gamma and beta\n",
        "            out = self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.view(-1)\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.view(-1)\n",
        "\n",
        "        else:\n",
        "            # Use stored running statistics during inference\n",
        "            x_norm = (x - self.running_mean.view(1, -1, 1, 1)) / torch.sqrt(self.running_var.view(1, -1, 1, 1) + self.eps)\n",
        "            out = self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "        # ğŸš§ Normalize and return the output ğŸ§ª\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vnc7yDv-BVG8"
      },
      "outputs": [],
      "source": [
        "class CustomReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    ğŸ› ï¸ TODO: Implement custom ReLU activation function âœ¨\n",
        "\n",
        "    ğŸ“œ **Requirements:**\n",
        "    1ï¸âƒ£ Apply ReLU manually using tensor operations (avoid using `F.relu`) ğŸ”„\n",
        "    2ï¸âƒ£ Output should replace all negative values with 0 (ReLU behavior) ğŸ§¹\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ğŸ”„ **TODO: Implement forward pass for ReLU**\n",
        "        # Hint: Use `torch.max` to replace all negative values with 0 ğŸ¯\n",
        "        return torch.max(x, torch.tensor(0.0, device = x.device))\n",
        "        # ğŸš§ Replace and return the ReLU-activated output âš¡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3vzCxebGBVG8"
      },
      "outputs": [],
      "source": [
        "class CustomMaxPooling2d(nn.Module):\n",
        "    \"\"\"\n",
        "    ğŸ› ï¸ TODO: Implement custom 2D MaxPooling layer ğŸŠ\n",
        "\n",
        "    ğŸ“œ **Requirements:**\n",
        "    1ï¸âƒ£ Implement a max-pooling operation with a given kernel size and stride ğŸ“\n",
        "    2ï¸âƒ£ Return the maximum value in each pooling window ğŸŒŠ\n",
        "    3ï¸âƒ£ Ensure it supports both training and evaluation modes ğŸ”„\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(CustomMaxPooling2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ğŸ”„ **TODO: Implement forward pass for max-pooling**\n",
        "        # Hint: Use `unfold` to break the input into windows and compute the max for each window ğŸ”\n",
        "        # Calculate output dimensions\n",
        "        N, C, H, W = x.shape\n",
        "        H_out = max(1, (H - self.kernel_size) // self.stride + 1)\n",
        "        W_out = max(1, (W - self.kernel_size) // self.stride + 1)\n",
        "\n",
        "        # Adjust kernel size and stride if input is too small\n",
        "        kernel_h = min(self.kernel_size, H)\n",
        "        kernel_w = min(self.kernel_size, W)\n",
        "        stride_h = min(self.stride, H)\n",
        "        stride_w = min(self.stride, W)\n",
        "\n",
        "        unfolded = x.unfold(2, kernel_h, stride_h).unfold(3, kernel_w, stride_w)\n",
        "        unfolded = unfolded.contiguous().view(N, C, H_out, W_out, kernel_h * kernel_w)\n",
        "        pooled = unfolded.max(dim=-1)[0]\n",
        "\n",
        "        return pooled\n",
        "        # ğŸš§ Pool and return the reduced output ğŸŠâ€â™‚ï¸\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1s-Oshf-BVG8"
      },
      "outputs": [],
      "source": [
        "class CustomVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomVGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: conv3-64 -> conv3-64 -> maxpool\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2: conv3-128 -> conv3-128 -> maxpool\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3: conv3-256 -> conv3-256 -> maxpool\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4: conv3-512 -> conv3-512 -> maxpool\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5: conv3-512 -> conv3-512 -> maxpool\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "         # Calculate flattened size for Linear layer input\n",
        "        self.flattened_size = 512 * 7 * 7\n",
        "\n",
        "        # Classifier layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size , 4096),   # Input size matches flattened feature map\n",
        "            CustomReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "\n",
        "            nn.Linear(4096 , 4096),\n",
        "            CustomReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "\n",
        "            nn.Linear(4096 , num_classes)    # Output layer for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G0BDB1U9BVG8"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ **Part 5: Training Functions**\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    ğŸ› ï¸ TODO: Implement training loop for one epoch ğŸ‹ï¸â€â™‚ï¸\n",
        "    \"\"\"\n",
        "    model.train()  # ğŸ“ˆ Switch to training mode\n",
        "    running_loss = 0.0  # ğŸ’° Track the cumulative loss\n",
        "    correct = 0  # âœ… Correct predictions counter\n",
        "    total = 0  # ğŸ“Š Total samples counter\n",
        "\n",
        "    for data, target in train_loader:  # ğŸ”„ Loop through batches\n",
        "        # ğŸ“Œ Your code here (e.g., forward pass, loss calculation, backward pass, optimizer step)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # ğŸ“Š Return average loss and accuracy for the epoch\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    ğŸ§ª TODO: Implement evaluation loop ğŸ”\n",
        "    \"\"\"\n",
        "    model.eval()  # ğŸ”• Switch to evaluation mode (no gradients)\n",
        "    test_loss = 0  # ğŸ’° Track cumulative test loss\n",
        "    correct = 0  # âœ… Correct predictions counter\n",
        "    total = 0  # ğŸ“Š Total samples counter\n",
        "\n",
        "    with torch.no_grad():  # ğŸš« No gradient calculation for evaluation\n",
        "        # ğŸ“Œ Your code here (e.g., forward pass, loss calculation, accuracy calculation)\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # ğŸ“Š Return average test loss and accuracy\n",
        "    return test_loss / len(test_loader), 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "voC8QT3QBVG8"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ **Part 6: Main Training Loop**\n",
        "\n",
        "def main():\n",
        "    # âš™ï¸ **Hyperparameters**\n",
        "    BATCH_SIZE = 16  # ğŸ“¦ Batch size for data loading\n",
        "    EPOCHS = 3  # ğŸ”„ Number of training epochs\n",
        "    LEARNING_RATE = 0.001  # ğŸš€ Learning rate for optimizer\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # âš¡ Use GPU if available\n",
        "\n",
        "    # ğŸ“Š **Load data**\n",
        "    train_loader, test_loader = load_mnist_data(BATCH_SIZE)\n",
        "\n",
        "    # ğŸ› ï¸ **Initialize model, criterion, optimizer**\n",
        "    model = CustomVGG16().to(DEVICE)  # ğŸ–¥ï¸ Move model to the selected device\n",
        "    criterion = nn.CrossEntropyLoss()  # ğŸ¯ Loss function for classification\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # ğŸš€ Adam optimizer for better convergence\n",
        "\n",
        "    # ğŸ”„ **Training loop**\n",
        "    train_losses = []  # ğŸ“‰ Track training losses\n",
        "    test_losses = []  # ğŸ“‰ Track test losses\n",
        "    train_accs = []  # ğŸ“Š Track training accuracy\n",
        "    test_accs = []  # ğŸ“Š Track test accuracy\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # ğŸ‹ï¸â€â™‚ï¸ **TODO: Implement main training loop**\n",
        "        print(f\"ğŸŒŸ Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2f}%, Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.2f}%\")\n",
        "\n",
        "    # ğŸ“ˆ **Plot results**\n",
        "    # ğŸ› ï¸ **TODO: Create loss and accuracy plots**\n",
        "    # Example: plt.plot(train_losses), plt.plot(test_losses), etc.\n",
        "    epochs_range = range(1, EPOCHS+1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, test_losses, label='Test Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_accs, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, test_accs, label='Test Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_2YOXN0BVG9",
        "outputId": "c2356516-572d-4642-a06e-1c1292a32c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸŒŸ Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cSWOuE6BVG9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
