{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLeiw-l6BVG5",
        "outputId": "add61ec1-d9fe-4c6f-97e9-05713db0708b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# 🚀 **Importing Helper Modules**\n",
        "\n",
        "import torch  # 🧠 Core PyTorch library for tensor operations and neural networks\n",
        "import torch.nn as nn  # 🏗️ Neural network components (layers, loss functions)\n",
        "import torch.optim as optim  # ⚙️ Optimization algorithms (SGD, Adam, etc.)\n",
        "import torchvision  # 🎨 Computer vision utilities and datasets\n",
        "import torchvision.transforms as transforms  # 🖼️ Data transformations (normalization, augmentation)\n",
        "from torch.utils.data import DataLoader  # 🚚 For loading and batching data\n",
        "import matplotlib.pyplot as plt  # 📊 Visualization for losses and accuracies\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import datasets\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FUtsc0UlBVG7"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_mnist_data(batch_size=64):\n",
        "    \"\"\"\n",
        "    📦 Load and preprocess the MNIST dataset.\n",
        "    📜 Returns: train_loader and test_loader 🎯\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # 🔄 Resize images to 224x224 pixels\n",
        "        transforms.ToTensor(),  # 🔄 Convert images to tensors 📊\n",
        "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # 🌈 Convert grayscale to RGB\n",
        "        transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))  # ⚖️ Normalize for RGB\n",
        "        #transforms.Normalize((0.1307,), (0.3081,)) #normalize for grayscale\n",
        "    ])\n",
        "\n",
        "    # 🛠️ Load MNIST training and test datasets 🖼️\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader  # 🚚 Return the loaders 📦\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AQpYDtVLBVG7"
      },
      "outputs": [],
      "source": [
        "# 🚀 **Part 2: Custom Dropout Implementation**\n",
        "\n",
        "class CustomDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    🛠️ TODO: Implement custom dropout layer 🎯\n",
        "\n",
        "    📜 **Requirements:**\n",
        "    1️⃣ Initialize with **dropout probability** `p` 🎲\n",
        "    2️⃣ Implement **forward pass** with proper scaling 🔄\n",
        "    3️⃣ **Only drop** units during **training** (`self.training` flag) 🏋️‍♂️\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.5):\n",
        "        super(CustomDropout, self).__init__()\n",
        "        # 🎲 Store dropout probability (p between 0 and 1)\n",
        "        self.p = p\n",
        "        pass  # 🚧 Initialization complete! Time to implement the logic 🛠️\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 🔄 **TODO: Implement forward pass**\n",
        "        if self.training:  # 🏋️‍♂️ Drop units only during training mode\n",
        "            mask = torch.bernoulli(torch.ones_like(x) * (1 - self.p))\n",
        "            x = x * mask / (1 - self.p)\n",
        "            # 🚧 Work in progress! Apply dropout logic 🧪\n",
        "        return x  # 🔄 Return the (possibly dropped) output ✨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DNpn47FQBVG7"
      },
      "outputs": [],
      "source": [
        "# 🚀 **Part 3: Custom BatchNorm2d Implementation**\n",
        "\n",
        "class CustomBatchNorm2d(nn.Module):\n",
        "    \"\"\"\n",
        "    🛠️ TODO: Implement custom 2D batch normalization 🔄\n",
        "\n",
        "    📜 **Requirements:**\n",
        "    1️⃣ Initialize **running mean**, **variance**, **gamma (scale)**, and **beta (shift)** ⚖️\n",
        "    2️⃣ Implement **forward pass** with proper normalization ✨\n",
        "    3️⃣ Track **running statistics** during training 📊\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
        "        super(CustomBatchNorm2d, self).__init__()\n",
        "        # 🛠️ **TODO: Initialize parameters and buffers**\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta  = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "        self.register_buffer('running_var', torch.ones(num_features))\n",
        "        # 🚧 Work in progress 🚀\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 🔄 **TODO: Implement forward pass for batch normalization**\n",
        "        # Steps:|\n",
        "        # 1️⃣ Calculate batch mean and variance 📊\n",
        "        # 2️⃣ Normalize the input 🎯\n",
        "        # 3️⃣ Apply learnable parameters (gamma and beta) ⚙️\n",
        "        # 4️⃣ Update running statistics during training 🏋️‍♂️\n",
        "        if self.training:\n",
        "            batch_mean = x.mean(dim=[0, 2, 3], keepdim=True)  # Mean over N, H, W\n",
        "            batch_var = x.var(dim=[0, 2, 3], unbiased=False, keepdim=True)  # Variance over N, H, W\n",
        "\n",
        "            # Normalize input\n",
        "            x_norm = (x - batch_mean) / torch.sqrt(batch_var + self.eps)\n",
        "\n",
        "            # Scale and shift using gamma and beta\n",
        "            out = self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean.view(-1)\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var.view(-1)\n",
        "\n",
        "        else:\n",
        "            # Use stored running statistics during inference\n",
        "            x_norm = (x - self.running_mean.view(1, -1, 1, 1)) / torch.sqrt(self.running_var.view(1, -1, 1, 1) + self.eps)\n",
        "            out = self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "        # 🚧 Normalize and return the output 🧪\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vnc7yDv-BVG8"
      },
      "outputs": [],
      "source": [
        "class CustomReLU(nn.Module):\n",
        "    \"\"\"\n",
        "    🛠️ TODO: Implement custom ReLU activation function ✨\n",
        "\n",
        "    📜 **Requirements:**\n",
        "    1️⃣ Apply ReLU manually using tensor operations (avoid using `F.relu`) 🔄\n",
        "    2️⃣ Output should replace all negative values with 0 (ReLU behavior) 🧹\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 🔄 **TODO: Implement forward pass for ReLU**\n",
        "        # Hint: Use `torch.max` to replace all negative values with 0 🎯\n",
        "        return torch.max(x, torch.tensor(0.0, device = x.device))\n",
        "        # 🚧 Replace and return the ReLU-activated output ⚡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3vzCxebGBVG8"
      },
      "outputs": [],
      "source": [
        "class CustomMaxPooling2d(nn.Module):\n",
        "    \"\"\"\n",
        "    🛠️ TODO: Implement custom 2D MaxPooling layer 🏊\n",
        "\n",
        "    📜 **Requirements:**\n",
        "    1️⃣ Implement a max-pooling operation with a given kernel size and stride 📐\n",
        "    2️⃣ Return the maximum value in each pooling window 🌊\n",
        "    3️⃣ Ensure it supports both training and evaluation modes 🔄\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, kernel_size=2, stride=2):\n",
        "        super(CustomMaxPooling2d, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 🔄 **TODO: Implement forward pass for max-pooling**\n",
        "        # Hint: Use `unfold` to break the input into windows and compute the max for each window 🔍\n",
        "        # Calculate output dimensions\n",
        "        N, C, H, W = x.shape\n",
        "        H_out = max(1, (H - self.kernel_size) // self.stride + 1)\n",
        "        W_out = max(1, (W - self.kernel_size) // self.stride + 1)\n",
        "\n",
        "        # Adjust kernel size and stride if input is too small\n",
        "        kernel_h = min(self.kernel_size, H)\n",
        "        kernel_w = min(self.kernel_size, W)\n",
        "        stride_h = min(self.stride, H)\n",
        "        stride_w = min(self.stride, W)\n",
        "\n",
        "        unfolded = x.unfold(2, kernel_h, stride_h).unfold(3, kernel_w, stride_w)\n",
        "        unfolded = unfolded.contiguous().view(N, C, H_out, W_out, kernel_h * kernel_w)\n",
        "        pooled = unfolded.max(dim=-1)[0]\n",
        "\n",
        "        return pooled\n",
        "        # 🚧 Pool and return the reduced output 🏊‍♂️\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1s-Oshf-BVG8"
      },
      "outputs": [],
      "source": [
        "class CustomVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomVGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1: conv3-64 -> conv3-64 -> maxpool\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(64),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2: conv3-128 -> conv3-128 -> maxpool\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(128),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3: conv3-256 -> conv3-256 -> maxpool\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(256),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4: conv3-512 -> conv3-512 -> maxpool\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5: conv3-512 -> conv3-512 -> maxpool\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            CustomBatchNorm2d(512),\n",
        "            CustomReLU(),\n",
        "            CustomMaxPooling2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "         # Calculate flattened size for Linear layer input\n",
        "        self.flattened_size = 512 * 7 * 7\n",
        "\n",
        "        # Classifier layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size , 4096),   # Input size matches flattened feature map\n",
        "            CustomReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "\n",
        "            nn.Linear(4096 , 4096),\n",
        "            CustomReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "\n",
        "            nn.Linear(4096 , num_classes)    # Output layer for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, start_dim=1)  # Flatten all dimensions except batch\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G0BDB1U9BVG8"
      },
      "outputs": [],
      "source": [
        "# 🚀 **Part 5: Training Functions**\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    🛠️ TODO: Implement training loop for one epoch 🏋️‍♂️\n",
        "    \"\"\"\n",
        "    model.train()  # 📈 Switch to training mode\n",
        "    running_loss = 0.0  # 💰 Track the cumulative loss\n",
        "    correct = 0  # ✅ Correct predictions counter\n",
        "    total = 0  # 📊 Total samples counter\n",
        "\n",
        "    for data, target in train_loader:  # 🔄 Loop through batches\n",
        "        # 📌 Your code here (e.g., forward pass, loss calculation, backward pass, optimizer step)\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # 📊 Return average loss and accuracy for the epoch\n",
        "    return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    🧪 TODO: Implement evaluation loop 🔍\n",
        "    \"\"\"\n",
        "    model.eval()  # 🔕 Switch to evaluation mode (no gradients)\n",
        "    test_loss = 0  # 💰 Track cumulative test loss\n",
        "    correct = 0  # ✅ Correct predictions counter\n",
        "    total = 0  # 📊 Total samples counter\n",
        "\n",
        "    with torch.no_grad():  # 🚫 No gradient calculation for evaluation\n",
        "        # 📌 Your code here (e.g., forward pass, loss calculation, accuracy calculation)\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    # 📊 Return average test loss and accuracy\n",
        "    return test_loss / len(test_loader), 100. * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "voC8QT3QBVG8"
      },
      "outputs": [],
      "source": [
        "# 🚀 **Part 6: Main Training Loop**\n",
        "\n",
        "def main():\n",
        "    # ⚙️ **Hyperparameters**\n",
        "    BATCH_SIZE = 16  # 📦 Batch size for data loading\n",
        "    EPOCHS = 3  # 🔄 Number of training epochs\n",
        "    LEARNING_RATE = 0.001  # 🚀 Learning rate for optimizer\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # ⚡ Use GPU if available\n",
        "\n",
        "    # 📊 **Load data**\n",
        "    train_loader, test_loader = load_mnist_data(BATCH_SIZE)\n",
        "\n",
        "    # 🛠️ **Initialize model, criterion, optimizer**\n",
        "    model = CustomVGG16().to(DEVICE)  # 🖥️ Move model to the selected device\n",
        "    criterion = nn.CrossEntropyLoss()  # 🎯 Loss function for classification\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # 🚀 Adam optimizer for better convergence\n",
        "\n",
        "    # 🔄 **Training loop**\n",
        "    train_losses = []  # 📉 Track training losses\n",
        "    test_losses = []  # 📉 Track test losses\n",
        "    train_accs = []  # 📊 Track training accuracy\n",
        "    test_accs = []  # 📊 Track test accuracy\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # 🏋️‍♂️ **TODO: Implement main training loop**\n",
        "        print(f\"🌟 Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # Train for one epoch\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, DEVICE)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accs.append(test_acc)\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.2f}%, Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.2f}%\")\n",
        "\n",
        "    # 📈 **Plot results**\n",
        "    # 🛠️ **TODO: Create loss and accuracy plots**\n",
        "    # Example: plt.plot(train_losses), plt.plot(test_losses), etc.\n",
        "    epochs_range = range(1, EPOCHS+1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, test_losses, label='Test Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, train_accs, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, test_accs, label='Test Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.xticks(epochs_range)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_2YOXN0BVG9",
        "outputId": "c2356516-572d-4642-a06e-1c1292a32c33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌟 Epoch 1/3\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cSWOuE6BVG9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
